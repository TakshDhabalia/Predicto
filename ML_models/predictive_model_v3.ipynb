{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report,accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are relative paths. Replace paths with your own paths\n",
    "TRANSACTION_DF_PATH = r\"..\\Data\\customer_transaction_info.json\" \n",
    "PRODUCT_INFO_PATH = r\"..\\Data\\product_info.json\"   \n",
    "CUSTOMER_INFO_PATH = r\"..\\Data\\customers_info.json\"\n",
    "ORDERS_RETURNED_PATH = r\"..\\Data\\orders_returned_info.json\"\n",
    "REGION_SELLER_INFO_PATH= r\"..\\Data\\region_seller_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdersReturned(file_path=ORDERS_RETURNED_PATH):\n",
    "    file_path = Path(file_path).resolve()\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as jsonFile:\n",
    "            jsonData = json.load(jsonFile)\n",
    "        \n",
    "\n",
    "        indexes = list(jsonData.keys())\n",
    "        \n",
    "        # Extracting column names\n",
    "        jsonItems = list(jsonData.items())\n",
    "        columns = list(jsonItems[0][1].keys())\n",
    "\n",
    "        # Creating empty pandas dataframe\n",
    "        order_returned_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        for i in range(0, len(jsonItems)):\n",
    "            itmeDict = dict(jsonItems[i][1].items())\n",
    "            for key,item in itmeDict.items():\n",
    "                order_returned_df.loc[i, key] = item\n",
    "        \n",
    "\n",
    "        return order_returned_df\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransactionDetails(json_file_path=TRANSACTION_DF_PATH):\n",
    "\n",
    "    # Using pathlib.Path to handle file paths\n",
    "    json_file_path = Path(json_file_path).resolve()\n",
    "\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # There are 3 fields in which data is stored in this json file: columns, index, data\n",
    "        # We will extract column names and indexes in seperate lists and for data, we will prepare a list of lists.\n",
    "        # It will help us to prepare a pandas dataframe to work with later on.\n",
    "\n",
    "        column_names = data['columns']\n",
    "        indexes = data['index']\n",
    "\n",
    "        # Extracting all data rows\n",
    "        dataObjets = data['data']\n",
    "        dataRows = []\n",
    "\n",
    "        for i in range(0, len(dataObjets)):\n",
    "            dataRows.append(dataObjets[i])\n",
    "\n",
    "        # Creating a pandas dataframe \n",
    "        customer_transaction_df = pd.DataFrame(dataRows, columns=column_names, index=indexes)\n",
    "\n",
    "        return customer_transaction_df\n",
    "\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProductDetails(file_path=PRODUCT_INFO_PATH):\n",
    "    file_path = Path(file_path).resolve()\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as jsonFile:\n",
    "            data = json.load(jsonFile)\n",
    "        \n",
    "        schemaFields = data['schema']['fields']\n",
    "        column_names = []\n",
    "\n",
    "        dataObjects = data['data']\n",
    "\n",
    "        for i in range(0, len(schemaFields)):\n",
    "            column_names.append(schemaFields[i]['name'])\n",
    "\n",
    "        product_df = pd.DataFrame(data=dataObjects, columns=column_names)\n",
    "\n",
    "        return product_df\n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCustomerInfo(file_path=CUSTOMER_INFO_PATH):\n",
    "\n",
    "    file_path = Path(file_path).resolve()\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as jsonFile:\n",
    "            data = json.load(jsonFile)\n",
    "\n",
    "        column_names = list(data[0].keys())\n",
    "        dataObjects = []\n",
    "\n",
    "        for i in range(0, len(data)):\n",
    "            temp = []\n",
    "            for keys in data[i]:\n",
    "                temp.append(data[i][keys])\n",
    "            \n",
    "            dataObjects.append(temp)\n",
    "        \n",
    "\n",
    "        customer_info_df = pd.DataFrame(data=dataObjects, columns=column_names)\n",
    "        return customer_info_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessProducts():\n",
    "    \"\"\" \n",
    "    From initial observations,some of the productIDs are repeating in this dataset, althought the product names are different. Since, they're creating ambiguity in transactional information, we came up with following modifications:\n",
    "\n",
    "    * Appended '-x' string at the end of each productID, where x is an integer. Initially, all 'unique' productIDs will have the x value to be 0.\n",
    "    * As we encounter duplicate entries of same productID, we will go on incrmenting value of 'x' \n",
    "    * This will be our variantID. \n",
    "\n",
    "    The way transaction data is being handled, there's no room for duplicate productID entries to exist for same orderID and customerID combination.\n",
    "\n",
    "    \"\"\"\n",
    "    product_info_df = getProductDetails(PRODUCT_INFO_PATH)\n",
    "\n",
    "    product_info_df_sorted = product_info_df.sort_values(by=\"Product ID\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "    def fixProductIDs():\n",
    "        productID_count = {}\n",
    "        modified_IDs = []\n",
    "\n",
    "        for index, row in product_info_df_sorted.iterrows():\n",
    "            # Get the corresponding productID for each row\n",
    "            productID = row[\"Product ID\"]\n",
    "\n",
    "            if productID in productID_count:\n",
    "                productID_count[productID] += 1\n",
    "            else:\n",
    "                productID_count[productID] = 0\n",
    "\n",
    "            modified_IDs.append(f\"{productID}-{productID_count[productID]}\")            \n",
    "        \n",
    "        product_info_df_sorted[\"variant_id\"] = modified_IDs\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    fixProductIDs()\n",
    "\n",
    "    product_info_df_sorted = product_info_df_sorted.rename(columns={'Product ID' : 'Old Product ID', 'variant_id': 'Product ID'})\n",
    "    return product_info_df_sorted\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessTransactions(transactions_info_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Some records were found to have same [OrderID, CustomerID, ProductID] combination with different values for Quantity\n",
    "    To resolve this issue, we decided to aggregate these differing features such as Profit, Quantity bought and Sales amount by summing them up\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        columns_to_check = [\n",
    "            \"Order ID\",\n",
    "            \"Order Date\",\n",
    "            \"Ship Date\",\n",
    "            \"Ship Mode\",\n",
    "            \"Customer ID\",\n",
    "            \"Product ID\",\n",
    "            \"Discount\",\n",
    "        ]\n",
    "        temp_merged_quantity_df = transactions_info_df.groupby(\n",
    "            columns_to_check, as_index=False\n",
    "        ).agg({\"Quantity\": \"sum\", \"Sales\": \"sum\", \"Profit\": \"sum\"})\n",
    "\n",
    "        transactions_info_df = temp_merged_quantity_df\n",
    "\n",
    "        \"\"\" \n",
    "        2] The order and shipping dates are in UNIX timestamp format. During model training, how to use these dates becomes a question. Instead, we will be converting these dates to the ususal date-time format, and extracting the no of days have passed since the particular item was last bought\n",
    "\n",
    "        This could prove to be crucial for predictive analytics.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        current_date = datetime.today().date()\n",
    "        transactions_info_df[\"Order Date\"] = transactions_info_df[\n",
    "            \"Order Date\"\n",
    "        ].apply(lambda x: pd.to_datetime(x, unit=\"ms\").strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "        transactions_info_df[\"Last Ordered\"] = transactions_info_df[\n",
    "            \"Order Date\"\n",
    "        ].apply(lambda x: (current_date - x).days)\n",
    "\n",
    "        transactions_info_df[\"Ship Date\"] = transactions_info_df[\n",
    "            \"Ship Date\"\n",
    "        ].apply(lambda x: pd.to_datetime(x, unit=\"ms\").strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "        # transactions_info_df = transactions_info_df.drop(\n",
    "        #     columns=[\"Order Date\", \"Ship Date\"]\n",
    "        # )\n",
    "\n",
    "        \"\"\" \n",
    "        Reading the orders_returned data into a dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "        orders_returned_df = getOrdersReturned(\n",
    "            ORDERS_RETURNED_PATH\n",
    "        )\n",
    "\n",
    "        temp_merged_df = transactions_info_df.merge(\n",
    "            orders_returned_df, on=\"Order ID\", how=\"outer\"\n",
    "        )\n",
    "\n",
    "        temp_merged_df[\"Returned\"] = temp_merged_df[\"Returned\"].fillna(\"No\")\n",
    "        transactions_info_df = temp_merged_df\n",
    "\n",
    "\n",
    "        \"\"\" \n",
    "        After we have created a variantID for each repeating productID, we need to make subsequent changes for productIDs in the transactions table too.\n",
    "        * Since there's no way as of now to understand the business logic behind the which product was bought from the products falling under the same productID, the idea is to replace it with first occurence of variantID in the product Info table\n",
    "    \n",
    "        \"\"\"\n",
    "        product_df = preProcessProducts()\n",
    "        variant_mappings = product_df.groupby(\"Old Product ID\")[\"Product ID\"].first().to_dict()\n",
    "        transactions_info_df[\"Product ID\"] = transactions_info_df[\"Product ID\"].map(variant_mappings).fillna(transactions_info_df[\"Product ID\"])\n",
    "\n",
    "        # Performing left join on transaction data with the product information\n",
    "\n",
    "        temp_joined_df = transactions_info_df.merge(product_df, on=\"Product ID\", how=\"inner\")\n",
    "        temp_joined_df = temp_joined_df.drop(columns=[\"Old Product ID\", \"index\"])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        transactions_info_df = temp_joined_df\n",
    "        \n",
    "\n",
    "        return transactions_info_df\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataFeatures(df, product_ids):\n",
    "    lc = LabelEncoder()\n",
    "    df[\"Customer ID\"] = lc.fit_transform(df[\"Customer ID\"])\n",
    "    customer_features = df.groupby(\"Customer ID\").agg({\n",
    "    'Sales' : ['sum', 'mean', 'count'],\n",
    "    'Profit' : ['sum', 'mean'],\n",
    "    'Discount' : ['mean'],\n",
    "    'Quantity' : ['sum', 'mean'],\n",
    "    'Returned' : lambda x : (x == \"Yes\").mean()\n",
    "}).reset_index()\n",
    "    \n",
    "    customer_features.columns = ['Customer ID'] + [\n",
    "        f'{col[0]}_{col[1]}'.lower() for col in customer_features.columns[1:]\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=[\"Order ID\"])\n",
    "\n",
    "    # Getting all customers \n",
    "    all_customers = df[\"Customer ID\"].unique()\n",
    "\n",
    "    # Filtering those customers which have bought products provided in the list of product_ids above.\n",
    "    target_purchases = df[df[\"Product ID\"].isin(product_ids)]\n",
    "    customers_with_target = target_purchases[\"Customer ID\"].unique()\n",
    "\n",
    "    y = pd.Series(index=all_customers, data=0)\n",
    "    y[customers_with_target] = 1\n",
    "\n",
    "\n",
    "    X = customer_features.set_index(\"Customer ID\")\n",
    "\n",
    "\n",
    "    # Renaming columns to ensure consistency while training XGBoost Model\n",
    "    X = X.rename(columns={'sales_sum' : 'salesSum', 'sales_mean': 'salesMean', 'sales_count': 'salesCount', 'profit_sum' : 'profitSum', \n",
    "                      'profit_mean': 'profitMean', 'discount_mean': 'discountMean', 'quantity_sum': 'quantitySum', 'quantity_mean': 'quantityMean', 'returned_<lambda>': 'returnedRate'})\n",
    "    \n",
    "    X = X.reindex(all_customers)\n",
    "    y = y.reindex(all_customers)\n",
    "\n",
    "    \n",
    "    return X, y, lc\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainXGBModel(X, y):\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                        random_state=42, \n",
    "                                                        stratify=y) \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    feature_names = X.columns\n",
    "\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_scaled, label=y_train, feature_names=list(X.columns))\n",
    "    dtest = xgb.DMatrix(X_test_scaled, label=y_test, feature_names=list(X.columns))\n",
    "\n",
    "    params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'max_depth': 6,\n",
    "            'eta': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'scale_pos_weight': sum(y_train == 0) / sum(y_train == 1) \n",
    "        }\n",
    "    \n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=100,\n",
    "        evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    y_pred = (model.predict(dtest) > 0.5).astype(int)\n",
    "\n",
    "    model_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return model, scaler, feature_names, model_accuracy\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSVM(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return model, scaler, X.columns, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRFModel(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    " \n",
    "    return model, scaler, X.columns, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRFPredictions(model, scaler, feature_names, customer_features):\n",
    "    X_scaled = scaler.transform(customer_features)\n",
    "    predictions = model.predict(X_scaled)\n",
    "    return pd.Series(predictions, index=customer_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(model, scaler, feature_names, customer_features):\n",
    "    # customer_features = customer_features[feature_names]\n",
    "    X_scaled = scaler.transform(customer_features)\n",
    "    dpredict = xgb.DMatrix(X_scaled, feature_names=list(feature_names))\n",
    "    probabilities = model.predict(dpredict)\n",
    "    return pd.Series(probabilities, index=customer_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSVMPredictions(model, scaler, feature_names, customer_features):\n",
    "    X_scaled = scaler.transform(customer_features)\n",
    "    predictions = model.predict(X_scaled)\n",
    "    return pd.Series(predictions, index=customer_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelatedProducts(product_info_df, keyword, top_n=30):\n",
    "\n",
    "\n",
    "    product_info_df[\"Product_info\"] = product_info_df[\"Category\"] + \" \" + product_info_df[\"Product Name\"] + \" \" + product_info_df[\"Sub-Category\"]\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=None, max_features=7000)\n",
    "    feature_vectors = vectorizer.fit_transform(product_info_df[\"Product_info\"])\n",
    "\n",
    "\n",
    "    keyword_vector = vectorizer.transform([keyword])\n",
    "\n",
    "    if keyword_vector.sum() == 0:\n",
    "        print(\"Warning: Keyword not found in vocabulary!\")\n",
    "        return []\n",
    "\n",
    "    similarities = cosine_similarity(keyword_vector, feature_vectors).flatten()\n",
    "\n",
    "\n",
    "    top_indices = similarities.argsort()[::-1][:top_n]\n",
    "\n",
    "    # results = [(transaction_df_2.iloc[i][\"Product ID\"], transaction_df_2.iloc[i][\"Product Name\"], similarities[i]) for i in top_indices]\n",
    "    results = [product_info_df.iloc[i][\"Product ID\"] for i in top_indices]\n",
    "    results = list(set(results))\n",
    "\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(customer_id, keyword):\n",
    "    df = getTransactionDetails(TRANSACTION_DF_PATH)\n",
    "    df = preProcessTransactions(df)\n",
    "\n",
    "    product_info_df = df.drop(columns=['Order ID', 'Ship Mode', 'Customer ID', 'Discount','Quantity', 'Sales', 'Profit', 'last_ordered', 'Returned'])\n",
    "\n",
    "    product_ids = getRelatedProducts(product_info_df, keyword)\n",
    "\n",
    "\n",
    "    X, y, customer_label_enc = prepareDataFeatures(df, product_ids)\n",
    "\n",
    "    model, scaler, feature_names, accuracy = trainXGBModel(X, y)\n",
    "\n",
    "    svmModel, svmScaler, svmFeatureNames, mse, r2 = trainSVM(X, y)\n",
    "\n",
    "    RFModel, RFscaler, RF_feature_names, mse_rf, r2_rf = trainRFModel(X, y)\n",
    "\n",
    "    probabilities = getPredictions(model, scaler, feature_names, X)\n",
    "    probailities_SVM = getSVMPredictions(svmModel, svmScaler, svmFeatureNames, X)\n",
    "    probabilities_RF = getRFPredictions(RFModel, RFscaler, RF_feature_names, X)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Mean-squared error of SVM Regressor: {mse}\\nR2-Score of SVM Regressor: {r2}\\n\\n\")\n",
    "    print(f\"Mean-squared error of Random Forest Regressor: {mse_rf}\\nR2-Score of Random Forest Regressor: {r2_rf}\\n\\n\")\n",
    "\n",
    "    print(f\"Accuracy score of XGBoost Model: {accuracy}\")\n",
    "    customer_id = int(customer_label_enc.transform([customer_id]))\n",
    "    \n",
    "    if customer_id in probabilities.index:\n",
    "        return probabilities[customer_id]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported operand type(s) for -: 'datetime.date' and 'str'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m probability \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDV-13045\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrinter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(probability)\n",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(customer_id, keyword)\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m getTransactionDetails(TRANSACTION_DF_PATH)\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m preProcessTransactions(df)\n\u001b[1;32m----> 5\u001b[0m product_info_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrder ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShip Mode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomer ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiscount\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProfit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_ordered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturned\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m product_ids \u001b[38;5;241m=\u001b[39m getRelatedProducts(product_info_df, keyword)\n\u001b[0;32m     10\u001b[0m X, y, customer_label_enc \u001b[38;5;241m=\u001b[39m prepareDataFeatures(df, product_ids)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "probability = main('DV-13045', 'Printer')\n",
    "print()\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>1478563200000</td>\n",
       "      <td>1478822400000</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>261.9600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>1478563200000</td>\n",
       "      <td>1478822400000</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>731.9400</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>219.5820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA-2016-138688</td>\n",
       "      <td>1465689600000</td>\n",
       "      <td>1466035200000</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>14.6200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.8714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>1444521600000</td>\n",
       "      <td>1445126400000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>957.5775</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-383.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>1444521600000</td>\n",
       "      <td>1445126400000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>22.3680</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.5164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>CA-2014-110422</td>\n",
       "      <td>1390262400000</td>\n",
       "      <td>1390435200000</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>TB-21400</td>\n",
       "      <td>FUR-FU-10001889</td>\n",
       "      <td>25.2480</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>CA-2017-121258</td>\n",
       "      <td>1488067200000</td>\n",
       "      <td>1488499200000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>DB-13060</td>\n",
       "      <td>FUR-FU-10000747</td>\n",
       "      <td>91.9600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.6332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>CA-2017-121258</td>\n",
       "      <td>1488067200000</td>\n",
       "      <td>1488499200000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>DB-13060</td>\n",
       "      <td>TEC-PH-10003645</td>\n",
       "      <td>258.5760</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>19.3932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>CA-2017-121258</td>\n",
       "      <td>1488067200000</td>\n",
       "      <td>1488499200000</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>DB-13060</td>\n",
       "      <td>OFF-PA-10004041</td>\n",
       "      <td>29.6000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>CA-2017-119914</td>\n",
       "      <td>1493856000000</td>\n",
       "      <td>1494288000000</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CC-12220</td>\n",
       "      <td>OFF-AP-10002684</td>\n",
       "      <td>243.1600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72.9480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9994 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Order ID     Order Date      Ship Date       Ship Mode  \\\n",
       "0     CA-2016-152156  1478563200000  1478822400000    Second Class   \n",
       "1     CA-2016-152156  1478563200000  1478822400000    Second Class   \n",
       "2     CA-2016-138688  1465689600000  1466035200000    Second Class   \n",
       "3     US-2015-108966  1444521600000  1445126400000  Standard Class   \n",
       "4     US-2015-108966  1444521600000  1445126400000  Standard Class   \n",
       "...              ...            ...            ...             ...   \n",
       "9989  CA-2014-110422  1390262400000  1390435200000    Second Class   \n",
       "9990  CA-2017-121258  1488067200000  1488499200000  Standard Class   \n",
       "9991  CA-2017-121258  1488067200000  1488499200000  Standard Class   \n",
       "9992  CA-2017-121258  1488067200000  1488499200000  Standard Class   \n",
       "9993  CA-2017-119914  1493856000000  1494288000000    Second Class   \n",
       "\n",
       "     Customer ID       Product ID     Sales  Quantity  Discount    Profit  \n",
       "0       CG-12520  FUR-BO-10001798  261.9600         2      0.00   41.9136  \n",
       "1       CG-12520  FUR-CH-10000454  731.9400         3      0.00  219.5820  \n",
       "2       DV-13045  OFF-LA-10000240   14.6200         2      0.00    6.8714  \n",
       "3       SO-20335  FUR-TA-10000577  957.5775         5      0.45 -383.0310  \n",
       "4       SO-20335  OFF-ST-10000760   22.3680         2      0.20    2.5164  \n",
       "...          ...              ...       ...       ...       ...       ...  \n",
       "9989    TB-21400  FUR-FU-10001889   25.2480         3      0.20    4.1028  \n",
       "9990    DB-13060  FUR-FU-10000747   91.9600         2      0.00   15.6332  \n",
       "9991    DB-13060  TEC-PH-10003645  258.5760         2      0.20   19.3932  \n",
       "9992    DB-13060  OFF-PA-10004041   29.6000         4      0.00   13.3200  \n",
       "9993    CC-12220  OFF-AP-10002684  243.1600         2      0.00   72.9480  \n",
       "\n",
       "[9994 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_transaction_df = getTransactionDetails(TRANSACTION_DF_PATH)\n",
    "customer_transaction_df = preProcessTransactions(customer_transaction_df)\n",
    "customer_transaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_transaction_info = getTransactionDetails(TRANSACTION_DF_PATH)\n",
    "customer_transaction_info = preProcessTransactions(customer_transaction_info)\n",
    "customer_transaction_info.to_csv(r'../Data/transactions_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmcenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
