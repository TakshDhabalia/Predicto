{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are relative paths. Replace paths with your own paths\n",
    "TRANSACTION_DF_PATH = r\"Data\\customer_transaction_info.json\" \n",
    "PRODUCT_INFO_PATH = r\"Data\\product_info.json\"   \n",
    "CUSTOMER_INFO_PATH = r\"Data\\customers_info.json\"\n",
    "ORDERS_RETURNED_PATH = r\"Data\\orders_returned_info.json\"\n",
    "REGION_SELLER_INFO_PATH= r\"Data\\region_seller_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdersReturned(file_path=ORDERS_RETURNED_PATH):\n",
    "    file_path = Path(file_path).resolve()\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as jsonFile:\n",
    "            jsonData = json.load(jsonFile)\n",
    "        \n",
    "\n",
    "        indexes = list(jsonData.keys())\n",
    "        \n",
    "        # Extracting column names\n",
    "        jsonItems = list(jsonData.items())\n",
    "        columns = list(jsonItems[0][1].keys())\n",
    "\n",
    "        # Creating empty pandas dataframe\n",
    "        order_returned_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        for i in range(0, len(jsonItems)):\n",
    "            itmeDict = dict(jsonItems[i][1].items())\n",
    "            for key,item in itmeDict.items():\n",
    "                order_returned_df.loc[i, key] = item\n",
    "        \n",
    "\n",
    "        return order_returned_df\n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransactionDetails(json_file_path=TRANSACTION_DF_PATH):\n",
    "\n",
    "    # Using pathlib.Path to handle file paths\n",
    "    json_file_path = Path(json_file_path).resolve()\n",
    "\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # There are 3 fields in which data is stored in this json file: columns, index, data\n",
    "        # We will extract column names and indexes in seperate lists and for data, we will prepare a list of lists.\n",
    "        # It will help us to prepare a pandas dataframe to work with later on.\n",
    "\n",
    "        column_names = data['columns']\n",
    "        indexes = data['index']\n",
    "\n",
    "        # Extracting all data rows\n",
    "        dataObjets = data['data']\n",
    "        dataRows = []\n",
    "\n",
    "        for i in range(0, len(dataObjets)):\n",
    "            dataRows.append(dataObjets[i])\n",
    "\n",
    "        # Creating a pandas dataframe \n",
    "        customer_transaction_df = pd.DataFrame(dataRows, columns=column_names, index=indexes)\n",
    "\n",
    "        return customer_transaction_df\n",
    "\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProductDetails(file_path=PRODUCT_INFO_PATH):\n",
    "    file_path = Path(file_path).resolve()\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as jsonFile:\n",
    "            data = json.load(jsonFile)\n",
    "        \n",
    "        schemaFields = data['schema']['fields']\n",
    "        column_names = []\n",
    "\n",
    "        dataObjects = data['data']\n",
    "\n",
    "        for i in range(0, len(schemaFields)):\n",
    "            column_names.append(schemaFields[i]['name'])\n",
    "\n",
    "        product_df = pd.DataFrame(data=dataObjects, columns=column_names)\n",
    "\n",
    "        return product_df\n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCustomerInfo(file_path=CUSTOMER_INFO_PATH):\n",
    "\n",
    "    file_path = Path(file_path).resolve()\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as jsonFile:\n",
    "            data = json.load(jsonFile)\n",
    "\n",
    "        column_names = list(data[0].keys())\n",
    "        dataObjects = []\n",
    "\n",
    "        for i in range(0, len(data)):\n",
    "            temp = []\n",
    "            for keys in data[i]:\n",
    "                temp.append(data[i][keys])\n",
    "            \n",
    "            dataObjects.append(temp)\n",
    "        \n",
    "\n",
    "        customer_info_df = pd.DataFrame(data=dataObjects, columns=column_names)\n",
    "        return customer_info_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessProducts():\n",
    "    \"\"\" \n",
    "    From initial observations,some of the productIDs are repeating in this dataset, althought the product names are different. Since, they're creating ambiguity in transactional information, we came up with following modifications:\n",
    "\n",
    "    * Appended '-x' string at the end of each productID, where x is an integer. Initially, all 'unique' productIDs will have the x value to be 0.\n",
    "    * As we encounter duplicate entries of same productID, we will go on incrmenting value of 'x' \n",
    "    * This will be our variantID. \n",
    "\n",
    "    The way transaction data is being handled, there's no room for duplicate productID entries to exist for same orderID and customerID combination.\n",
    "\n",
    "    \"\"\"\n",
    "    product_info_df = getProductDetails(PRODUCT_INFO_PATH)\n",
    "\n",
    "    product_info_df_sorted = product_info_df.sort_values(by=\"Product ID\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "    def fixProductIDs():\n",
    "        productID_count = {}\n",
    "        modified_IDs = []\n",
    "\n",
    "        for index, row in product_info_df_sorted.iterrows():\n",
    "            # Get the corresponding productID for each row\n",
    "            productID = row[\"Product ID\"]\n",
    "\n",
    "            if productID in productID_count:\n",
    "                productID_count[productID] += 1\n",
    "            else:\n",
    "                productID_count[productID] = 0\n",
    "\n",
    "            modified_IDs.append(f\"{productID}-{productID_count[productID]}\")            \n",
    "        \n",
    "        product_info_df_sorted[\"variant_id\"] = modified_IDs\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    fixProductIDs()\n",
    "\n",
    "    product_info_df_sorted = product_info_df_sorted.rename(columns={'Product ID' : 'Old Product ID', 'variant_id': 'Product ID'})\n",
    "    return product_info_df_sorted\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessTransactions(transactions_info_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Some records were found to have same [OrderID, CustomerID, ProductID] combination with different values for Quantity\n",
    "    To resolve this issue, we decided to aggregate these differing features such as Profit, Quantity bought and Sales amount by summing them up\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        columns_to_check = [\n",
    "            \"Order ID\",\n",
    "            \"Order Date\",\n",
    "            \"Ship Date\",\n",
    "            \"Ship Mode\",\n",
    "            \"Customer ID\",\n",
    "            \"Product ID\",\n",
    "            \"Discount\",\n",
    "        ]\n",
    "        temp_merged_quantity_df = transactions_info_df.groupby(\n",
    "            columns_to_check, as_index=False\n",
    "        ).agg({\"Quantity\": \"sum\", \"Sales\": \"sum\", \"Profit\": \"sum\"})\n",
    "\n",
    "        transactions_info_df = temp_merged_quantity_df\n",
    "\n",
    "        \"\"\" \n",
    "        2] The order and shipping dates are in UNIX timestamp format. During model training, how to use these dates becomes a question. Instead, we will be converting these dates to the ususal date-time format, and extracting the no of days have passed since the particular item was last bought\n",
    "\n",
    "        This could prove to be crucial for predictive analytics.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        current_date = datetime.today().date()\n",
    "        transactions_info_df[\"Order_date_modified\"] = transactions_info_df[\n",
    "            \"Order Date\"\n",
    "        ].apply(lambda x: pd.to_datetime(x, unit=\"ms\").date())\n",
    "\n",
    "        transactions_info_df[\"last_ordered\"] = transactions_info_df[\n",
    "            \"Order_date_modified\"\n",
    "        ].apply(lambda x: (current_date - x).days)\n",
    "\n",
    "        transactions_info_df = transactions_info_df.drop(\n",
    "            columns=[\"Order_date_modified\", \"Order Date\", \"Ship Date\"]\n",
    "        )\n",
    "\n",
    "        \"\"\" \n",
    "        Reading the orders_returned data into a dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "        orders_returned_df = getOrdersReturned(\n",
    "            ORDERS_RETURNED_PATH\n",
    "        )\n",
    "\n",
    "        temp_merged_df = transactions_info_df.merge(\n",
    "            orders_returned_df, on=\"Order ID\", how=\"outer\"\n",
    "        )\n",
    "\n",
    "        temp_merged_df[\"Returned\"] = temp_merged_df[\"Returned\"].fillna(\"No\")\n",
    "        transactions_info_df = temp_merged_df\n",
    "\n",
    "\n",
    "        \"\"\" \n",
    "        After we have created a variantID for each repeating productID, we need to make subsequent changes for productIDs in the transactions table too.\n",
    "        * Since there's no way as of now to understand the business logic behind the which product was bought from the products falling under the same productID, the idea is to replace it with first occurence of variantID in the product Info table\n",
    "    \n",
    "        \"\"\"\n",
    "        product_df = preProcessProducts()\n",
    "        variant_mappings = product_df.groupby(\"Old Product ID\")[\"Product ID\"].first().to_dict()\n",
    "        transactions_info_df[\"Product ID\"] = transactions_info_df[\"Product ID\"].map(variant_mappings).fillna(transactions_info_df[\"Product ID\"])\n",
    "\n",
    "        # Performing left join on transaction data with the product information\n",
    "\n",
    "        temp_joined_df = transactions_info_df.merge(product_df, on=\"Product ID\", how=\"inner\")\n",
    "        temp_joined_df = temp_joined_df.drop(columns=[\"Old Product ID\", \"index\"])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        transactions_info_df = temp_joined_df\n",
    "        \n",
    "\n",
    "        return transactions_info_df\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataFeatures(df, product_ids):\n",
    "    lc = LabelEncoder()\n",
    "    df[\"Customer ID\"] = lc.fit_transform(df[\"Customer ID\"])\n",
    "    customer_features = df.groupby(\"Customer ID\").agg({\n",
    "    'Sales' : ['sum', 'mean', 'count'],\n",
    "    'Profit' : ['sum', 'mean'],\n",
    "    'Discount' : ['mean'],\n",
    "    'Quantity' : ['sum', 'mean'],\n",
    "    'Returned' : lambda x : (x == \"Yes\").mean()\n",
    "}).reset_index()\n",
    "    \n",
    "    customer_features.columns = ['Customer ID'] + [\n",
    "        f'{col[0]}_{col[1]}'.lower() for col in customer_features.columns[1:]\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=[\"Order ID\"])\n",
    "\n",
    "    # Getting all customers \n",
    "    all_customers = df[\"Customer ID\"].unique()\n",
    "\n",
    "    # Filtering those customers which have bought products provided in the list of product_ids above.\n",
    "    target_purchases = df[df[\"Product ID\"].isin(product_ids)]\n",
    "    customers_with_target = target_purchases[\"Customer ID\"].unique()\n",
    "\n",
    "    y = pd.Series(index=all_customers, data=0)\n",
    "    y[customers_with_target] = 1\n",
    "\n",
    "\n",
    "    X = customer_features.set_index(\"Customer ID\")\n",
    "\n",
    "\n",
    "    # Renaming columns to ensure consistency while training XGBoost Model\n",
    "    X = X.rename(columns={'sales_sum' : 'salesSum', 'sales_mean': 'salesMean', 'sales_count': 'salesCount', 'profit_sum' : 'profitSum', \n",
    "                      'profit_mean': 'profitMean', 'discount_mean': 'discountMean', 'quantity_sum': 'quantitySum', 'quantity_mean': 'quantityMean', 'returned_<lambda>': 'returnedRate'})\n",
    "    \n",
    "    X = X.reindex(all_customers)\n",
    "    y = y.reindex(all_customers)\n",
    "\n",
    "    \n",
    "    return X, y, lc\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainXGBModel(X, y):\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                        random_state=42, \n",
    "                                                        stratify=y) \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    feature_names = X.columns\n",
    "\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_scaled, label=y_train, feature_names=list(X.columns))\n",
    "    dtest = xgb.DMatrix(X_test_scaled, label=y_test, feature_names=list(X.columns))\n",
    "\n",
    "    params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'max_depth': 6,\n",
    "            'eta': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'scale_pos_weight': sum(y_train == 0) / sum(y_train == 1) \n",
    "        }\n",
    "    \n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=100,\n",
    "        evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    y_pred = (model.predict(dtest) > 0.5).astype(int)\n",
    "\n",
    "    model_accuracy = accuracy_score(y_test, y_pred)\n",
    "    return model, scaler, feature_names, model_accuracy\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(model, scaler, feature_names, customer_features):\n",
    "    # customer_features = customer_features[feature_names]\n",
    "    X_scaled = scaler.transform(customer_features)\n",
    "    dpredict = xgb.DMatrix(X_scaled, feature_names=list(feature_names))\n",
    "    probabilities = model.predict(dpredict)\n",
    "    return pd.Series(probabilities, index=customer_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelatedProducts(product_info_df, keyword, top_n=30):\n",
    "\n",
    "\n",
    "    product_info_df[\"Product_info\"] = product_info_df[\"Category\"] + \" \" + product_info_df[\"Product Name\"] + \" \" + product_info_df[\"Sub-Category\"]\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=None, max_features=7000)\n",
    "    feature_vectors = vectorizer.fit_transform(product_info_df[\"Product_info\"])\n",
    "\n",
    "\n",
    "    keyword_vector = vectorizer.transform([keyword])\n",
    "\n",
    "    if keyword_vector.sum() == 0:\n",
    "        print(\"Warning: Keyword not found in vocabulary!\")\n",
    "        return []\n",
    "\n",
    "    similarities = cosine_similarity(keyword_vector, feature_vectors).flatten()\n",
    "\n",
    "\n",
    "    top_indices = similarities.argsort()[::-1][:top_n]\n",
    "\n",
    "    # results = [(transaction_df_2.iloc[i][\"Product ID\"], transaction_df_2.iloc[i][\"Product Name\"], similarities[i]) for i in top_indices]\n",
    "    results = [product_info_df.iloc[i][\"Product ID\"] for i in top_indices]\n",
    "    results = list(set(results))\n",
    "\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(customer_id, keyword):\n",
    "    df = getTransactionDetails(TRANSACTION_DF_PATH)\n",
    "    df = preProcessTransactions(df)\n",
    "\n",
    "    product_info_df = df.drop(columns=['Order ID', 'Ship Mode', 'Customer ID', 'Discount','Quantity', 'Sales', 'Profit', 'last_ordered', 'Returned'])\n",
    "\n",
    "    product_ids = getRelatedProducts(product_info_df, keyword)\n",
    "\n",
    "\n",
    "    X, y, customer_label_enc = prepareDataFeatures(df, product_ids)\n",
    "\n",
    "    model, scaler, feature_names, accuracy = trainXGBModel(X, y)\n",
    "\n",
    "    probabilities = getPredictions(model, scaler, feature_names, X)\n",
    "\n",
    "\n",
    "    customer_id = int(customer_label_enc.transform([customer_id]))\n",
    "    \n",
    "    if customer_id in probabilities.index:\n",
    "        return probabilities[customer_id]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46084386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athug\\AppData\\Local\\Temp\\ipykernel_8552\\959590710.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  customer_id = int(customer_label_enc.transform([customer_id]))\n"
     ]
    }
   ],
   "source": [
    "probability = main('LB-16795', 'Printer')\n",
    "print(probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmcenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
